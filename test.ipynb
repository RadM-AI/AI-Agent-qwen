{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8300f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 38.50it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name,force_download=True, use_default_system_prompt=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efeaf003",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python311-64\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f152919",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 46.05it/s]\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    \"Qwen/Qwen2.5-3B-Instruct\",\n",
    "    temperature=0.6,\n",
    "    max_new_tokens=32000,\n",
    "    device=\"cuda:0\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf114388",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.config.prompts import main_prompt\n",
    "from src.tools.registry import ToolRegistry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b27c5de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.tools.structured.StructuredTool'>\n",
      "<class 'langchain_core.tools.structured.StructuredTool'>\n",
      "<class 'langchain_core.tools.structured.StructuredTool'>\n"
     ]
    }
   ],
   "source": [
    "prompt = main_prompt.format(tools_description = ToolRegistry().get_tools_description(), history = '', input = \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d45be82",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": prompt},\n",
    "    {\"role\": \"user\", \"content\": 'how 1000*123/333333'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c348706",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m text = \u001b[43mtokenizer\u001b[49m.apply_chat_template(\n\u001b[32m      2\u001b[39m     messages,\n\u001b[32m      3\u001b[39m     tokenize=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m      4\u001b[39m     add_generation_prompt=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m      5\u001b[39m     enable_thinking=\u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;66;03m# Switches between thinking and non-thinking modes. Default is True.\u001b[39;00m\n\u001b[32m      6\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "51ca4834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "how 1000*123/333333<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "bc21394a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "064170ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=32768\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e9799f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a96289fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # rindex finding 151668 (</think>)\n",
    "    index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "except ValueError:\n",
    "    index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "fa7b00ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "899db3e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thinking content: \n",
      "content: To solve the expression \\(1000 \\times 123 / 333333\\), you can follow these steps:\n",
      "\n",
      "1. First, multiply 1000 by 123:\n",
      "   \\[\n",
      "   1000 \\times 123 = 123000\n",
      "   \\]\n",
      "\n",
      "2. Next, divide the result by 333333:\n",
      "   \\[\n",
      "   123000 / 333333 \\approx 0.36894736842105263\n",
      "   \\]\n",
      "\n",
      "So, \\(1000 \\times 123 / 333333\\) is approximately \\(0.36894736842105263\\).\n"
     ]
    }
   ],
   "source": [
    "print(\"thinking content:\", thinking_content)\n",
    "print(\"content:\", content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7bb285ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline, ChatHuggingFace\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.messages import SystemMessage, HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f626c47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = ChatHuggingFace(llm=HuggingFacePipeline(pipeline=pipe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c092ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate(\n",
    "    template='{input}',\n",
    "    input_variables=['input'],\n",
    "    \n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "816f8535",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = SystemMessage(content=\"четкий поц\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec1525a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nПривет!<|im_end|>\\n<|im_start|>assistant\\nЗдравствуйте! Как я могу вам помочь сегодня?', additional_kwargs={}, response_metadata={}, id='run--b85a8844-be69-410c-ae24-784faec7265b-0')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(prompt_template | m).invoke({\"input\": \"Привет!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f1e25a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['\"tool\"', \"'title'\"], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['\"tool\"', \"'title'\"], input_types={}, partial_variables={}, template='You are an intelligent assistant with access to the following tools:\\n\\nHISTORY:\\n\\n\\nYOUR AVAILABLE TOOLS:\\nget_weather:\\n   - Description: Get the current weather for the specified city.\\n        Args:\\n            input_data: The name of the city in English (for example: \\'Moscow\\', \\'Samara\\')\\n\\n        Returns:\\n            A line with information about temperature and weather conditions\\n        \\n   - Input data: input: {\\'title\\': \\'Input\\'}\\n\\ncalculate:\\n   - Description: Calculate a mathematical expression.\\n        Args:\\n            input_data: A mathematical expression (for example: \\'2 + 2 * 3\\', \\'sin(45)\\')\\n\\n        Returns:\\n            Calculation result\\n        \\n   - Input data: input: {\\'title\\': \\'Input\\'}\\n\\nsearch_information:\\n   - Description: Search for information on a given query.\\n        Args:\\n            input_data: A search query in Russian\\n\\n        Returns:\\n            Information found\\n        \\n   - Input data: input: {\\'title\\': \\'Input\\'}\\n\\n\\nINSTRUCTIONS FOR USE:\\n1. Carefully analyze the user\\'s request\\n  2. Determine if you need a response tool.\\n  3. Use ONLY the tools from the list of AVAILABLE TOOLS.\\n  4. If you NEED a tool to respond, start the response with the [TOOL] tag and then place the JSON object.\\n  5. If you DON\\'T NEED the tool, reply in plain text.\\n\\nexamples:\\n\\nQuery: \"Какая погода в Москве?\"\\nResponse: [TOOL] {\"tool\": \"get_weather\", \"input\": \"Moscow\"}\\n\\nRequest: \"Привет! Как дела?\"\\nAnswer: Привет! Я ИИ ассистент. Чем могу помочь?\\n\\nSTRICT RULES:\\n- DON\\'T FORGET that YOU can communicate with the user and keep the conversation going.\\n- NEVER write explanations before or after JSON\\n- NEVER use keys other than \"tool\", \"input\"\\n- NEVER come up with new tool names - use ONLY AVAILABLE TOOLS from the list.\\n- There must be a [TOOL] BEFORE the JSON.\\n- ANSWER IN RUSSIAN\\n\\n\\n\\nUSER\\'S QUESTION: \\n\\n'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='Привет!'), additional_kwargs={})])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "047abb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline, ChatHuggingFace\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "m = ChatHuggingFace(llm=HuggingFacePipeline(pipeline=pipe))\n",
    "\n",
    "# Создаем чат-промпт с системным сообщением\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"Ты - экспертный AI ассистент по программированию. Ты помогаешь с кодом на Python.\"),\n",
    "    HumanMessage(content=\"{input}\")\n",
    "])\n",
    "\n",
    "# Создаем цепочку\n",
    "chain = chat_prompt | m\n",
    "\n",
    "# Используем\n",
    "response = chain.invoke({\"input\": \"Как написать функцию на Python?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0fe40c82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='<|im_start|>system\\nТы - экспертный AI ассистент по программированию. Ты помогаешь с кодом на Python.<|im_end|>\\n<|im_start|>user\\n{input}<|im_end|>\\n<|im_start|>assistant\\nКажется, вы забыли указать что именно вам нужна помощь. Вы можете вставить свой код или вопрос о том, как решить определенную задачу. Если у вас есть конкретный код или проблема, я буду рад помочь!', additional_kwargs={}, response_metadata={}, id='run--e1eaa6b0-f7f5-495e-90f2-c2276423a2e8-0')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da62adb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
